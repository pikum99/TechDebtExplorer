./python_file_to_csv.py,36,"import csv
import os


def get_python_files(directory):
    for root, dirs, files in os.walk(directory):
        for file in files:
            if file.endswith('.py'):
                yield os.path.join(root, file)


def get_file_content(file_path):
    with open(file_path, 'r') as file:
        return file.readlines()


def write_to_csv(file_path, data):
    with open(file_path, 'w', newline='') as file:
        writer = csv.writer(file)
        writer.writerows(data)


def main():
    directory = './'  # フォルダを指定
    output_csv = './crawling_output/output.csv'  # 出力するCSVファイル名を指定

    data = []
    for file_path in get_python_files(directory):
        content = get_file_content(file_path)
        data.append([file_path, len(content), ''.join(content)])

    write_to_csv(output_csv, data)


if __name__ == ""__main__"":
    main()
"
./analyze_code_similarity.py,105,"import ast
import Levenshtein
import csv


def generate_ast(code):
    return ast.parse(code)


def calculate_edit_distance(ast1, ast2):
    # ここでASTの比較を行う
    # 例: ast.dump(ast1) と ast.dump(ast2) を比較する
    return Levenshtein.distance(ast.dump(ast1), ast.dump(ast2))


def group_codes_by_similarity(codes, threshold):
    groups = []

    for i, code1 in enumerate(codes):
        ast1 = generate_ast(code1)

        # 新しいグループを作成
        new_group = [i]

        for j, code2 in enumerate(codes):
            if i != j:  # 同じコードとの比較は不要
                ast2 = generate_ast(code2)
                distance = calculate_edit_distance(ast1, ast2)
                if distance <= threshold:
                    new_group.append(j)
        # 既存のグループに含まれるかどうかを確認
        merged = False
        for group in groups:
            if any(index in group for index in new_group):
                group.update(new_group)
                merged = True
                break

        # 新しいグループを追加
        if not merged:
            groups.append(set(new_group))
    return [list(group) for group in groups]


def calculate_group_distances(grouped_codes, codes):
    group_distances = []

    for group in grouped_codes:
        distances = []
        for i in range(len(group)):
            for j in range(i+1, len(group)):
                ast1 = generate_ast(codes[group[i]])
                ast2 = generate_ast(codes[group[j]])
                distance = calculate_edit_distance(ast1, ast2)
                distances.append(distance)
        # グループ内の平均編集距離を計算
        avg_distance = sum(distances) / len(distances) if distances else 0
        group_distances.append(avg_distance)

    return group_distances


def calculate_inter_group_distances(grouped_codes, codes):
    inter_group_distances = []

    # 各グループの「中心」を計算
    group_centers = []
    for group in grouped_codes:
        asts = [generate_ast(codes[i]) for i in group]
        ast_strs = [ast.dump(a) for a in asts]
        center = sum(len(ast_str) for ast_str in ast_strs) / len(ast_strs)
        group_centers.append(center)

    # すべてのグループペアの中心間の編集距離を計算
    for i in range(len(group_centers)):
        for j in range(i+1, len(group_centers)):
            distance = abs(group_centers[i] - group_centers[j])
            inter_group_distances.append(distance)

    return inter_group_distances


# CSVファイルを開く
with open('crawling_output/output.csv', 'r') as f:
    reader = csv.reader(f)
    # リストに変換
    codes = [row[2] for row in reader]

# 確認のために出力
# print(codes)

threshold = 2000  # 編集距離の閾値

# ASTを生成してグループ化
grouped_codes = group_codes_by_similarity(codes, threshold)

# グループ内の平均編集距離を計算
group_distances = calculate_group_distances(grouped_codes, codes)

# グループ間の平均編集距離を計算
inter_group_distances = calculate_inter_group_distances(grouped_codes, codes)

print(""Grouped Codes:"", grouped_codes)
print(""Group Distances:"", group_distances)
print(""Inter-Group Distances:"", inter_group_distances)
"
./tutorial_file/test4.py,94,"import ast
import Levenshtein

def generate_ast(code):
    return ast.parse(code)

def calculate_edit_distance(ast1, ast2):
    return Levenshtein.distance(ast.dump(ast1), ast.dump(ast2))

def calculate_group_center(group, codes):
    # グループの中心点として各ASTの平均を計算
    center_ast = ast.parse("""")
    for index in group:
        center_ast = ast.fix_missing_locations(
            ast.increment_lineno(
                ast.fix_missing_locations(
                    ast.Module(body=center_ast.body + generate_ast(codes[index]).body)
                )
            )
        )
    
    return center_ast

def group_codes_by_similarity(codes, threshold):
    groups = []

    for i, code1 in enumerate(codes):
        ast1 = generate_ast(code1)
        
        # 新しいグループを作成
        new_group = [i]
        
        for j, code2 in enumerate(codes):
            if i != j:  # 同じコードとの比較は不要
                ast2 = generate_ast(code2)
                distance = calculate_edit_distance(ast1, ast2)
                
                if distance <= threshold:
                    new_group.append(j)
        
        # 既存のグループに含まれるかどうかを確認
        merged = False
        for group in groups:
            if any(index in group for index in new_group):
                group.update(new_group)
                merged = True
                break
        
        # 新しいグループを追加
        if not merged:
            groups.append(set(new_group))
    
    return [list(group) for group in groups]

def calculate_distances_to_center(grouped_codes, codes):
    distances = {}
    
    for group in grouped_codes:
        center_ast = calculate_group_center(group, codes)
        
        for index in group:
            code_ast = generate_ast(codes[index])
            distance_to_center = calculate_edit_distance(center_ast, code_ast)
            distances[index] = distance_to_center
    
    return distances

# テスト用のコードサンプル
code1 = """"""
class UserDetailSerializer(serializers.ModelSerializer):
    class Meta:
        model = User
        fields = ('id', 'show_name', 'images', 'self_intro', 'search_word_1', 'search_word_2', 'search_word_3')
""""""
code2 = """"""
class CharacterSerializer(serializers.ModelSerializer):
    class Meta:
        model = Character
        fields = '__all__'
""""""
code3 = ""for i in range(10): print(i)""
code4 = ""print('Hello, world!')""

codes = [code1, code2, code3, code4]
threshold = 5  # 編集距離の閾値

# ASTを生成してグループ化
grouped_codes = group_codes_by_similarity(codes, threshold)

# 各コードとそのグループの中心点との距離を計算
distances_to_center = calculate_distances_to_center(grouped_codes, codes)

print(""Grouped Codes:"", grouped_codes)
print(""Distances to Center:"", distances_to_center)
"
./tutorial_file/test.py,41,"import ast
import Levenshtein

def generate_ast(source_code):
    """"""
    ソースコードからASTを生成する関数
    """"""
    try:
        parsed_ast = ast.parse(source_code)
        return parsed_ast
    except SyntaxError as e:
        print(f""SyntaxError: {e}"")
        return None

def calculate_ast_edit_distance(ast1, ast2):
    """"""
    2つのASTの編集距離を計算する関数
    """"""
    if ast1 is None or ast2 is None:
        return float('inf')  # エラーケースなどで距離が無限大になるようにする

    # ASTを文字列に変換してLevenshtein距離を計算
    ast_str1 = ast.dump(ast1)
    ast_str2 = ast.dump(ast2)
    edit_distance = Levenshtein.distance(ast_str1, ast_str2)

    return edit_distance

source_code1 = ""def add_numbers(a, b):""
source_code2 = ""def add_numbers(a, b):""


# ASTを生成
ast1 = generate_ast(source_code1)
ast2 = generate_ast(source_code2)

# ASTの編集距離を計算
edit_distance = calculate_ast_edit_distance(ast1, ast2)

# 結果を出力
print(f""AST Edit Distance: {edit_distance}"")
"
./tutorial_file/test2.py,42,"def generate_ngrams(tokens, N):
    """"""
    トークン列からN-gramsを生成する関数
    """"""
    ngrams = []
    for i in range(len(tokens) - N + 1):
        ngram = tuple(tokens[i:i + N])
        ngrams.append(ngram)
    return ngrams

def calculate_ngram_similarity(code1, code2, N):
    """"""
    2つのコードのN-gram類似度を計算する関数
    """"""
    # コードをトークンに分割
    tokens1 = code1.split()
    tokens2 = code2.split()

    # N-gramsを生成
    ngrams1 = generate_ngrams(tokens1, N)
    ngrams2 = generate_ngrams(tokens2, N)

    # 共通のN-gramsを抽出
    common_ngrams = set(ngrams1) & set(ngrams2)

    # 類似度を計算
    similarity = len(common_ngrams) / max(len(ngrams1), len(ngrams2))

    return similarity

# 例として2つのコードを用意
code1 = ""def add_numbers(a, b): return a + b""
code2 = ""def sum_numbers(x, y): return x + y""

# N-gramの次元（ここでは2-gram）
N = 2

# N-gram類似度を計算
similarity = calculate_ngram_similarity(code1, code2, N)

# 結果を出力
print(f""{N}-gram Similarity: {similarity}"")
"
./tutorial_file/test3.py,55,"import ast
import Levenshtein

def generate_ast(code):
    return ast.parse(code)

def calculate_edit_distance(ast1, ast2):
    # ここでASTの比較を行う
    # 例: ast.dump(ast1) と ast.dump(ast2) を比較する
    return Levenshtein.distance(ast.dump(ast1), ast.dump(ast2))

def group_codes_by_similarity(codes, threshold):
    groups = []

    for i, code1 in enumerate(codes):
        ast1 = generate_ast(code1)
        
        # 新しいグループを作成
        new_group = [i]
        
        for j, code2 in enumerate(codes):
            if i != j:  # 同じコードとの比較は不要
                ast2 = generate_ast(code2)
                distance = calculate_edit_distance(ast1, ast2)
                
                if distance <= threshold:
                    new_group.append(j)
        
        # 既存のグループに含まれるかどうかを確認
        merged = False
        for group in groups:
            if any(index in group for index in new_group):
                group.update(new_group)
                merged = True
                break
        
        # 新しいグループを追加
        if not merged:
            groups.append(set(new_group))
    
    return [list(group) for group in groups]

# テスト用のコードサンプル
code1 = ""print('Hello, world!')""
code2 = ""print('Hi there!')""
code3 = ""for i in range(10): print(i)""
code4 = ""print('Hello, world!')""

codes = [code1, code2, code3, code4]
threshold = 5  # 編集距離の閾値

# ASTを生成してグループ化
grouped_codes = group_codes_by_similarity(codes, threshold)

print(""Grouped Codes:"", grouped_codes)
"
